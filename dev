#!/bin/bash
# Local development script for Dagster pipelines
#
# Usage:
#   ./dev up           - Start Dagster dev environment (Docker)
#   ./dev down         - Stop all containers
#   ./dev new <name>   - Create a new pipeline from template
#   ./dev build        - Rebuild base Docker image
#   ./dev logs [svc]   - View logs (optionally for specific service)
#   ./dev db           - Open interactive DuckDB session (local)
#   ./dev db-remote    - Open interactive DuckDB session (remote)
#   ./dev export <tbl> - Export table to file (default: local, parquet)
#   ./dev pull         - List remote tables
#   ./dev pull <table> - Pull table from remote DuckLake
#   ./dev mark <asset> - Mark asset as materialized

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

# Ensure data directory exists for local DuckLake
mkdir -p "$SCRIPT_DIR/data"

DEV_COMPOSE="docker-compose.dev.generated.yaml"

case "$1" in
    up)
        # Load environment for validation
        if [ ! -f ".env.local" ]; then
            echo "Error: .env.local not found. Copy .env.example to .env.local and configure it."
            exit 1
        fi

        # Build base image if needed
        if ! docker image inspect my-platform-base:latest > /dev/null 2>&1; then
            echo "Building base image..."
            docker build -t my-platform-base:latest -f Dockerfile.base .
        fi

        # Generate configs (workspace + dev compose)
        echo "Generating configs..."
        uv run python3 generate_platform.py

        echo ""
        echo "Starting Dagster dev environment..."
        echo "Open http://localhost:3000 in your browser"
        echo ""
        echo "Hot reload: Edit code, then click 'Reload' in the Dagster UI"
        echo ""
        docker compose -f "$DEV_COMPOSE" up --build
        ;;

    new)
        # Create a new pipeline from template
        NAME="$2"
        if [ -z "$NAME" ]; then
            echo "Usage: ./dev new <pipeline_name>"
            echo ""
            echo "Example: ./dev new my_pipeline"
            exit 1
        fi

        # Validate name (lowercase, underscores only)
        if [[ ! "$NAME" =~ ^[a-z][a-z0-9_]*$ ]]; then
            echo "Error: Pipeline name must be lowercase, start with a letter, and contain only letters, numbers, and underscores."
            exit 1
        fi

        DEST="pipelines/$NAME"
        if [ -d "$DEST" ]; then
            echo "Error: Pipeline '$NAME' already exists at $DEST"
            exit 1
        fi

        echo "Creating new pipeline: $NAME"

        # Copy template
        cp -r pipelines/_template "$DEST"

        # Update Dockerfile
        sed -i "s/my_pipeline/$NAME/g" "$DEST/Dockerfile"

        # Update pyproject.toml
        sed -i "s/my-pipeline/$NAME-pipeline/g" "$DEST/pyproject.toml"

        # Update defs.py group name
        sed -i "s/my_pipeline/$NAME/g" "$DEST/defs.py"

        echo ""
        echo "Pipeline '$NAME' created at $DEST"
        echo ""
        echo "Next steps:"
        echo "  1. Add your assets in $DEST/assets/"
        echo "  2. Update $DEST/defs.py to import and register your assets"
        echo "  3. Add any dependencies to $DEST/pyproject.toml"
        echo "  4. Run ./dev up to start the dev environment"
        ;;

    build)
        # Rebuild base image
        echo "Rebuilding base image..."
        docker build -t my-platform-base:latest -f Dockerfile.base .
        ;;

    down)
        # Stop all containers
        docker compose -f "$DEV_COMPOSE" down
        ;;

    logs)
        # View logs
        SERVICE="$2"
        if [ -z "$SERVICE" ]; then
            docker compose -f "$DEV_COMPOSE" logs -f
        else
            docker compose -f "$DEV_COMPOSE" logs -f "$SERVICE"
        fi
        ;;
    
    mark)
        # Mark assets as materialized without running them
        if [ -f ".env.local" ]; then
            set -a
            source .env.local
            set +a
        fi

        ASSETS="$2"
        if [ -z "$ASSETS" ]; then
            echo "Usage: ./dev mark <asset1,asset2,...>"
            echo ""
            echo "Example: ./dev mark hackernews_top_stories,hackernews_titles"
            exit 1
        fi

        # Run inside the webserver container to use the correct Dagster instance
        docker compose -f "$DEV_COMPOSE" exec dagster_webserver python -c "
from dagster import DagsterInstance, AssetKey
from dagster import AssetMaterialization

instance = DagsterInstance.get()

assets = '$ASSETS'.split(',')
for asset_name in assets:
    asset_name = asset_name.strip()
    key = AssetKey(asset_name)
    
    instance.report_runless_asset_event(
        AssetMaterialization(
            asset_key=key,
            description='Manually marked as materialized',
            metadata={'source': 'dev mark'}
        )
    )
    print(f'Marked {asset_name} as materialized')
"
        ;;

    db)
        # Open interactive DuckDB session with local and remote DuckLake attached
        if [ -f ".env.local" ]; then
            set -a
            source .env.local
            set +a
        fi

        uv run duckdb -cmd "
INSTALL ducklake; INSTALL postgres; INSTALL httpfs;
LOAD ducklake; LOAD postgres; LOAD httpfs;

SET s3_region = '${DUCKLAKE_REMOTE_S3_REGION:-us-east-1}';
SET s3_endpoint = '${DUCKLAKE_REMOTE_S3_ENDPOINT}';
SET s3_url_style = '${DUCKLAKE_REMOTE_S3_URL_STYLE:-path}';
SET s3_access_key_id = '${DUCKLAKE_REMOTE_S3_ACCESS_KEY_ID}';
SET s3_secret_access_key = '${DUCKLAKE_REMOTE_S3_SECRET_ACCESS_KEY}';
SET s3_use_ssl = ${DUCKLAKE_REMOTE_S3_USE_SSL:-false};

ATTACH 'ducklake:${DUCKLAKE_CATALOG_DSN}' AS local (DATA_PATH '${DUCKLAKE_DATA_PATH}');
ATTACH 'ducklake:postgres:${DUCKLAKE_REMOTE_CATALOG_DSN}' AS remote (DATA_PATH '${DUCKLAKE_REMOTE_DATA_PATH}', READ_ONLY);
USE local;
"
        ;;

    db-remote)
        # Open interactive DuckDB session with remote DuckLake attached
        if [ -f ".env.local" ]; then
            set -a
            source .env.local
            set +a
        fi

        if [ -z "$DUCKLAKE_REMOTE_CATALOG_DSN" ]; then
            echo "Error: DUCKLAKE_REMOTE_CATALOG_DSN not set in .env.local"
            exit 1
        fi

        uv run duckdb -cmd "
INSTALL ducklake; INSTALL postgres; INSTALL httpfs;
LOAD ducklake; LOAD postgres; LOAD httpfs;

SET s3_region = '${DUCKLAKE_REMOTE_S3_REGION:-us-east-1}';
SET s3_endpoint = '${DUCKLAKE_REMOTE_S3_ENDPOINT}';
SET s3_url_style = '${DUCKLAKE_REMOTE_S3_URL_STYLE:-path}';
SET s3_access_key_id = '${DUCKLAKE_REMOTE_S3_ACCESS_KEY_ID}';
SET s3_secret_access_key = '${DUCKLAKE_REMOTE_S3_SECRET_ACCESS_KEY}';
SET s3_use_ssl = ${DUCKLAKE_REMOTE_S3_USE_SSL:-false};

ATTACH 'ducklake:postgres:${DUCKLAKE_REMOTE_CATALOG_DSN}' AS remote (DATA_PATH '${DUCKLAKE_REMOTE_DATA_PATH}');
USE remote;
"
        ;;

    export)
        # Export table to local file
        if [ -f ".env.local" ]; then
            set -a
            source .env.local
            set +a
        fi

        TABLE="$2"
        FORMAT="${3:-parquet}"
        SOURCE="${4:-local}"

        if [ -z "$TABLE" ]; then
            echo "Usage: ./dev export <table> [format] [source]"
            echo ""
            echo "Formats: parquet (default), csv, json, ndjson"
            echo "Source:  local (default), remote"
            echo ""
            echo "Examples:"
            echo "  ./dev export customers                  # Export local as parquet"
            echo "  ./dev export customers csv              # Export local as CSV"
            echo "  ./dev export customers parquet remote   # Export remote as parquet"
            exit 1
        fi

        # Determine file extension and format options
        case "$FORMAT" in
            parquet)
                EXT="parquet"
                FORMAT_OPTS="FORMAT PARQUET"
                ;;
            csv)
                EXT="csv"
                FORMAT_OPTS="FORMAT CSV, HEADER"
                ;;
            json)
                EXT="json"
                FORMAT_OPTS="FORMAT JSON, ARRAY true"
                ;;
            ndjson)
                EXT="ndjson"
                FORMAT_OPTS="FORMAT JSON"
                ;;
            *)
                echo "Error: Unknown format '$FORMAT'"
                echo "Supported formats: parquet, csv, json, ndjson"
                exit 1
                ;;
        esac

        if [ "$SOURCE" = "remote" ]; then
            if [ -z "$DUCKLAKE_REMOTE_CATALOG_DSN" ]; then
                echo "Error: DUCKLAKE_REMOTE_CATALOG_DSN not set in .env.local"
                exit 1
            fi

            SQL="
INSTALL ducklake; INSTALL postgres; INSTALL httpfs;
LOAD ducklake; LOAD postgres; LOAD httpfs;

SET s3_region = '${DUCKLAKE_REMOTE_S3_REGION:-us-east-1}';
SET s3_endpoint = '${DUCKLAKE_REMOTE_S3_ENDPOINT}';
SET s3_url_style = '${DUCKLAKE_REMOTE_S3_URL_STYLE:-path}';
SET s3_access_key_id = '${DUCKLAKE_REMOTE_S3_ACCESS_KEY_ID}';
SET s3_secret_access_key = '${DUCKLAKE_REMOTE_S3_SECRET_ACCESS_KEY}';
SET s3_use_ssl = ${DUCKLAKE_REMOTE_S3_USE_SSL:-false};

ATTACH 'ducklake:postgres:${DUCKLAKE_REMOTE_CATALOG_DSN}' AS remote (DATA_PATH '${DUCKLAKE_REMOTE_DATA_PATH}', READ_ONLY);

COPY (SELECT * FROM remote.${TABLE}) TO '${TABLE}.${EXT}' (${FORMAT_OPTS});
SELECT '${TABLE}.${EXT}: ' || COUNT(*) || ' rows exported from remote' FROM remote.${TABLE};
"
        else
            SQL="
INSTALL ducklake; LOAD ducklake;

ATTACH 'ducklake:${DUCKLAKE_CATALOG_DSN}' AS local (DATA_PATH '${DUCKLAKE_DATA_PATH}');

COPY (SELECT * FROM local.${TABLE}) TO '${TABLE}.${EXT}' (${FORMAT_OPTS});
SELECT '${TABLE}.${EXT}: ' || COUNT(*) || ' rows exported from local' FROM local.${TABLE};
"
        fi

        uv run duckdb -c "$SQL" 2>/dev/null | tail -n +3
        ;;

    pull)
        # Pull tables from remote DuckLake to local DuckLake
        if [ -f ".env.local" ]; then
            set -a
            source .env.local
            set +a
        else
            echo "Error: .env.local not found."
            exit 1
        fi

        if [ -z "$DUCKLAKE_REMOTE_CATALOG_DSN" ]; then
            echo "Error: DUCKLAKE_REMOTE_CATALOG_DSN not set in .env.local"
            echo ""
            echo "Configure remote DuckLake connection in .env.local:"
            echo "  DUCKLAKE_REMOTE_CATALOG_DSN=\"dbname=... host=... user=... password=... port=...\""
            echo "  DUCKLAKE_REMOTE_DATA_PATH=s3://bucket/path/"
            echo "  DUCKLAKE_REMOTE_S3_ENDPOINT=..."
            echo "  DUCKLAKE_REMOTE_S3_ACCESS_KEY_ID=..."
            echo "  DUCKLAKE_REMOTE_S3_SECRET_ACCESS_KEY=..."
            exit 1
        fi

        TABLE="$2"
        LIMIT="$3"

        # Build DuckDB SQL - remote uses Postgres+S3, local uses file-based
        SQL="
INSTALL ducklake; INSTALL postgres; INSTALL httpfs;
LOAD ducklake; LOAD postgres; LOAD httpfs;

-- Remote S3 config
SET s3_region = '${DUCKLAKE_REMOTE_S3_REGION:-us-east-1}';
SET s3_endpoint = '${DUCKLAKE_REMOTE_S3_ENDPOINT}';
SET s3_url_style = '${DUCKLAKE_REMOTE_S3_URL_STYLE:-path}';
SET s3_access_key_id = '${DUCKLAKE_REMOTE_S3_ACCESS_KEY_ID}';
SET s3_secret_access_key = '${DUCKLAKE_REMOTE_S3_SECRET_ACCESS_KEY}';
SET s3_use_ssl = ${DUCKLAKE_REMOTE_S3_USE_SSL:-false};

-- Attach remote (Postgres + S3)
ATTACH 'ducklake:postgres:${DUCKLAKE_REMOTE_CATALOG_DSN}' AS remote (DATA_PATH '${DUCKLAKE_REMOTE_DATA_PATH}', READ_ONLY);

-- Attach local (file-based)
ATTACH 'ducklake:${DUCKLAKE_CATALOG_DSN}' AS local (DATA_PATH '${DUCKLAKE_DATA_PATH}');
"

        if [ -z "$TABLE" ]; then
            # List tables
            SQL="${SQL}
SELECT name FROM (SHOW TABLES FROM remote);
"
            echo "Remote tables:"
            uv run duckdb -noheader -list -c "$SQL" 2>/dev/null
        else
            # Pull table
            if [ -n "$LIMIT" ]; then
                SQL="${SQL}
DROP TABLE IF EXISTS local.${TABLE};
CREATE TABLE local.${TABLE} AS SELECT * FROM remote.${TABLE} LIMIT ${LIMIT};
SELECT '${TABLE}: ' || COUNT(*) || ' rows pulled' FROM local.${TABLE};
"
            else
                SQL="${SQL}
DROP TABLE IF EXISTS local.${TABLE};
CREATE TABLE local.${TABLE} AS SELECT * FROM remote.${TABLE};
SELECT '${TABLE}: ' || COUNT(*) || ' rows pulled' FROM local.${TABLE};
"
            fi
            uv run duckdb -c "$SQL" 2>/dev/null | tail -n +3

            # Mark matching asset as materialized if it exists (run inside Docker)
            docker compose -f "$DEV_COMPOSE" exec -T dagster_webserver python -c "
from dagster import DagsterInstance, AssetKey, AssetMaterialization

instance = DagsterInstance.get()
asset_name = '${TABLE}'
key = AssetKey(asset_name)

try:
    instance.report_runless_asset_event(
        AssetMaterialization(
            asset_key=key,
            description='Pulled from remote DuckLake',
            metadata={'source': 'dev pull'}
        )
    )
    print(f'Marked {asset_name} as materialized')
except Exception as e:
    pass
" 2>/dev/null || true
        fi
        ;;

    *)
        echo "Usage: ./dev <command>"
        echo ""
        echo "Commands:"
        echo "  up                    Start Dagster dev environment (Docker)"
        echo "  down                  Stop all containers"
        echo "  new <name>            Create a new pipeline from template"
        echo "  build                 Rebuild base Docker image"
        echo "  logs [service]        View logs (optionally for specific service)"
        echo "  db                    Open interactive DuckDB session (local)"
        echo "  db-remote             Open interactive DuckDB session (remote)"
        echo "  export <tbl> [fmt] [src]  Export table to file (fmt: parquet/csv/json/ndjson, src: local/remote)"
        echo "  pull                  List remote tables"
        echo "  pull <table> [limit]  Pull table from remote (optionally with row limit)"
        echo "  mark <assets>         Mark assets as materialized (comma-separated)"
        echo ""
        echo "Examples:"
        echo "  ./dev up                       # Start Dagster"
        echo "  ./dev down                     # Stop containers"
        echo "  ./dev new my_pipeline          # Create new pipeline"
        echo "  ./dev build                    # Rebuild base image"
        echo "  ./dev logs pipeline_hackernews # View pipeline logs"
        echo "  ./dev pull                     # List remote tables"
        echo "  ./dev pull my_table            # Pull entire table"
        echo "  ./dev pull my_table 1000       # Pull 1000 rows"
        echo "  ./dev mark asset1,asset2       # Mark as materialized"
        ;;
esac
